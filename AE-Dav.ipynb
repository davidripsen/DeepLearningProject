{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae2c1cf-fe91-4fe9-bdfd-d9386aa8af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_style(\"whitegrid\")\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c291f2-b419-4e69-a813-6e057b6c942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pickle.load( open( \"data/df_merged.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42c90f0-c654-485e-b576-81d5e4357b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "cuda = False\n",
    "# define size variables\n",
    "num_features = 1\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_units, latent_features=2):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # We typically employ an \"hourglass\" structure\n",
    "        # meaning that the decoder should be an encoder\n",
    "        # in reverse.\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=hidden_units),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.Linear(in_features=hidden_units, out_features=hidden_units), # Extra layer\n",
    "            # bottleneck layer\n",
    "            nn.Linear(in_features=hidden_units, out_features=latent_features)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=hidden_units),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.Linear(in_features=hidden_units, out_features=hidden_units), # Extra layer\n",
    "            # output layer, projecting back to image size\n",
    "            nn.Linear(in_features=hidden_units, out_features=num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        outputs = {}\n",
    "        # we don't apply an activation to the bottleneck layer\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # apply sigmoid to output to get pixel intensities between 0 and 1\n",
    "        x_hat = torch.sigmoid(self.decoder(z))\n",
    "        \n",
    "        return {\n",
    "            'z': z,\n",
    "            'x_hat': x_hat\n",
    "        }\n",
    "\n",
    "\n",
    "# Choose the shape of the autoencoder\n",
    "net = AutoEncoder(hidden_units=128, latent_features=2) # Originalt: Hidden units = 128\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b1a6a0-2192-488f-8855-3057ff1b1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# if you want L2 regularization, then add weight_decay to SGD\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# We will use pixel wise mean-squared error as our loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7e17c3-2a56-48c8-af71-2dfca82ecbfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wv/j_lqm8yn4ndb1hvhthjcrb540000gn/T/ipykernel_9559/1359316757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Go through each batch in the training dataset using the loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Note that y is not necessarily known as it is here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_loss = []\n",
    "    net.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        #if cuda:\n",
    "        #    x = x.cuda() \n",
    "        outputs = net(x)\n",
    "        x_hat = outputs['x_hat']\n",
    "\n",
    "        # note, target is the original tensor, as we're working with auto-encoders\n",
    "        loss = loss_function(x_hat, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "\n",
    "    # Evaluate, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        # Just load a single batch from the test loader\n",
    "        x, y = next(iter(test_loader))\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        outputs = net(x)\n",
    "\n",
    "        # We save the latent variable and reconstruction for later use\n",
    "        # we will need them on the CPU to plot\n",
    "        x_hat = outputs['x_hat']\n",
    "        z = outputs['z'].cpu().numpy()\n",
    "\n",
    "        loss = loss_function(x_hat, x)\n",
    "\n",
    "        valid_loss.append(loss.item())\n",
    "    \n",
    "    if epoch == 0:\n",
    "        continue\n",
    "\n",
    "    # live plotting of the trainig curves and representation\n",
    "    #plot_autoencoder_stats(x=x,\n",
    "    #                       x_hat=x_hat,\n",
    "    #                       z=z,\n",
    "    #                       y=y,\n",
    "    #                       train_loss=train_loss,\n",
    "    #                       valid_loss=valid_loss,\n",
    "    #                       epoch=epoch,\n",
    "    #                       classes=classes,\n",
    "    #                       dimensionality_reduction_op=None) # = lambda z: TSNE(n_components=2).fit_transform(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782156d8-ebfd-48b9-8158-1e4d972a7592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
